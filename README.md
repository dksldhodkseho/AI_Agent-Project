# AIë©´ì ‘ê´€ Agent 

> GPT ê¸°ë°˜ AI ë©´ì ‘ê´€ Agent: ì´ë ¥ì„œ ë¶„ì„ â†’ ì§ˆë¬¸ ìƒì„± â†’ ë‹µë³€ í‰ê°€ â†’ í”¼ë“œë°± ì œê³µ

---

## âœ… í”„ë¡œì íŠ¸ ê°œ

ì´ í”„ë¡œì íŠ¸ëŠ” GPT ê¸°ë°˜ ë©´ì ‘ ì‹œë®¬ë ˆì´í„° Agentë¥¼ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- ì´ë ¥ì„œ ë¶„ì„ (ìš”ì•½ + í‚¤ì›Œë“œ + íŠ¸ë¦¬ê±° í¬ì¸íŠ¸ ì¶”ì¶œ)
- ì§ˆë¬¸ ì „ëµ ìˆ˜ë¦½ ë° ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±
- ë‹µë³€ì— ëŒ€í•œ í‰ê°€ ë° í”¼ë“œë°±
- ì¸í„°ë·° íë¦„ ì œì–´ ë° ì¢…í•© í‰ê°€ ì œê³µ
- Gradio ì›¹ ì•± ì—°ë™

---

## ğŸ› ï¸ ì‚¬ìš© ê¸°ìˆ 

- Python (Google Colab)
- LangChain, LangGraph
- OpenAI GPT-4o-mini
- Gradio (HuggingFace)
- Chroma (Vector DB)
  
---

## ğŸ§  ì‹œìŠ¤í…œ êµ¬ì¡°

AI ë©´ì ‘ê´€ì˜ ì „ì²´ íë¦„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

![ì‹œìŠ¤í…œ êµ¬ì¡°ë„](./ai_interview_pipeline.png)

> ìê¸°ì†Œê°œ â†’ AI ì§ˆë¬¸ ìƒì„± â†’ ë‹µë³€ ì…ë ¥ â†’ í”¼ë“œë°± ì œê³µ â†’ ì ìˆ˜í™”

---

## âš™ï¸ ì‹¤í–‰ ë°©ë²•

### 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ë™

```python
from google.colab import drive
drive.mount('/content/drive')
```
### 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```python
!pip install -r /content/drive/MyDrive/project_genai/requirements.txt
```
### 3. OpenAI API Key ì„¤ì •
api_key.txt íŒŒì¼ í˜•ì‹:
```python
OPENAI_API_KEY=your_api_key_here
```
Python ì½”ë“œë¡œ í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°:
```python
import os

def load_api_keys(filepath="api_key.txt"):
    with open(filepath, "r") as f:
        for line in f:
            if "=" in line:
                k, v = line.strip().split("=", 1)
                os.environ[k] = v

load_api_keys('/content/drive/MyDrive/project_genai/api_key.txt')
```
### 4. ì‹¤í–‰ ì˜ˆì‹œ
```python
filepath = '/content/drive/MyDrive/project_genai/Resume_sample.pdf'
state = preProcessing_Interview(filepath)

while True:
    print("[ì§ˆë¬¸]")
    print(state["current_question"])
    state["current_answer"] = input("[ë‹µë³€ ì…ë ¥]: ")
    state = graph.invoke(state)
    if state["next_step"] == "end":
        break
```

## ğŸ” ì£¼ìš” í•¨ìˆ˜ ì„¤ëª…

í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ê¸°ëŠ¥ì„ êµ¬ì„±í•˜ëŠ” ì£¼ìš” í•¨ìˆ˜ë“¤ì„ ê°„ë‹¨í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì†Œê°œí•©ë‹ˆë‹¤.

---

### 1. `extract_text_from_file(file_path)`

> ì´ë ¥ì„œ íŒŒì¼(PDF ë˜ëŠ” DOCX)ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

```python
import fitz  # PyMuPDF
from docx import Document
import os

def extract_text_from_file(file_path: str) -> str:
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".pdf":
        doc = fitz.open(file_path)
        text = "\n".join(page.get_text() for page in doc)
        doc.close()
        return text
    elif ext == ".docx":
        doc = Document(file_path)
        return "\n".join(p.text for p in doc.paragraphs if p.text.strip())
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
```

**ì‚¬ìš© ì˜ˆ:**
```python
text = extract_text_from_file('/content/drive/MyDrive/project_genai/Resume_sample.pdf')
```

---

### 2. `analyze_resume(state)`

> ì´ë ¥ì„œë¥¼ ë¶„ì„í•´ ìš”ì•½, í‚¤ì›Œë“œ, ì¤‘ìš”ë„, íŠ¸ë¦¬ê±° í¬ì¸íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.

```python
from pydantic import BaseModel
from langchain.output_parsers import PydanticOutputParser

class ResumeInfo(BaseModel):
    summary: str
    keywords: list[str]
    keyword_weights: dict[str, str]
    triggers: list[str]

def analyze_resume(state: InterviewState) -> InterviewState:
    parser = PydanticOutputParser(pydantic_object=ResumeInfo)
    # LLM prompt ìƒëµ
    response = llm.invoke(messages)
    resume_info = parser.parse(response.content)

    return {
        **state,
        "resume_summary": resume_info.summary,
        "resume_keywords": resume_info.keywords,
        "keyword_weights": resume_info.keyword_weights,
        "triggers": resume_info.triggers
    }
```

**ì‚¬ìš© ì˜ˆ:**
```python
state = analyze_resume(state)
```

---

### 3. `generate_question_strategy(state)`

> ì§ˆë¬¸ ì „ëµì„ 3ê°œ ë¶„ì•¼(ê²½ë ¥, ë™ê¸°, ì‚¬ê³ ë ¥)ë¡œ êµ¬ë¶„í•´ ë°©í–¥ê³¼ ì˜ˆì‹œ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

```python
class Strategy(BaseModel):
    questions: dict[str, dict]

def generate_question_strategy(state: InterviewState) -> InterviewState:
    parser = PydanticOutputParser(pydantic_object=Strategy)
    # LLM prompt ìƒëµ
    response = llm.invoke(messages)
    strategy = parser.parse(response.content)
    
    return {
        **state,
        "question_strategy": strategy.questions
    }
```

**ì‚¬ìš© ì˜ˆ:**
```python
state = generate_question_strategy(state)
```

---

### 4. `evaluate_answer(state)`

> GPTë¥¼ í†µí•´ ì‚¬ìš©ìì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³ , ê´€ë ¨ì„±, êµ¬ì²´ì„±, ì¢…í•© í‰ê°€ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.

```python
class Evaluation(BaseModel):
    content: dict[str, str]

def evaluate_answer(state: InterviewState) -> InterviewState:
    parser = PydanticOutputParser(pydantic_object=Evaluation)
    # LLM prompt ìƒëµ
    response = llm.invoke(messages)
    evaluation = parser.parse(response.content).content
    state["conversation"].append(evaluation)
    return {
        **state,
        "evaluation": [evaluation]
    }
```

**ì‚¬ìš© ì˜ˆ:**
```python
state["current_answer"] = "ì œê°€ ì§„í–‰í•œ í”„ë¡œì íŠ¸ëŠ”..."
state = evaluate_answer(state)
```

---

### 5. `decide_next_step(state)`

> ì „ëµ ì‚¬ìš© ì—¬ë¶€ ë° í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ì§ˆë¬¸ íë¦„ì„ ê²°ì •í•©ë‹ˆë‹¤.

```python
def decide_next_step(state: InterviewState) -> InterviewState:
    if len(state["conversation"]) >= 5:
        next_step = "end"
    else:
        last_eval = state["conversation"][-1]
        if last_eval["ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±"] in ["ì¤‘(ë³´í†µ)", "ìƒ(ìš°ìˆ˜)"]:
            next_step = "next_strategy"
        else:
            next_step = "additional_question"
    return { **state, "next_step": next_step }
```

**ì‚¬ìš© ì˜ˆ:**
```python
state = decide_next_step(state)
```

---

### 6. `generate_question(state)`

> í‰ê°€ ê²°ê³¼ì™€ ì „ëµì— ë”°ë¼ ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤. Vector DB ìœ ì‚¬ ì§ˆë¬¸ë„ í™œìš©í•©ë‹ˆë‹¤.

```python
def generate_question(state: InterviewState) -> InterviewState:
    query = f"{state['current_strategy']}, {state['resume_keywords']}"
    similar_qs = vectordb.similarity_search(query, k=2)
    reference_questions = '\n'.join(f"- {doc.page_content}" for doc in similar_qs)
    # LLM prompt ìƒëµ
    response = llm.invoke(messages)

    return {
        **state,
        "current_question": response.content.strip()
    }
```

**ì‚¬ìš© ì˜ˆ:**
```python
state = generate_question(state)
```

---

### 7. `summarize_interview(state)`

> ë©´ì ‘ ì „ì²´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ëµë³„ ìƒì„¸ í”¼ë“œë°±ê³¼ ì¢…í•© í‰ê°€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
class TotalFeedbackInfo(BaseModel):
    feedback: str

def summarize_interview(state: InterviewState) -> InterviewState:
    feedbacks = []
    for evaluate in state["conversation"]:
        feedback = generate_feedback_paragraph(
            evaluate["ì§ˆë¬¸"], evaluate["ë‹µë³€"],
            evaluate["í‰ê°€ì¢…í•©"], evaluate["í‰ê°€ì— ëŒ€í•œ ì´ìœ "]
        )
        evaluate["ìƒì„¸í”¼ë“œë°±"] = feedback
        feedbacks.append(feedback)
    
    summary = llm.invoke([...])
    state["total_feedback"] = summary.content
    return state
```

**ì‚¬ìš© ì˜ˆ:**
```python
state = summarize_interview(state)
print(state["total_feedback"])
```

---

ì´ í•¨ìˆ˜ë“¤ì€ LangGraphë¥¼ í†µí•´ ìƒíƒœ ê¸°ë°˜ìœ¼ë¡œ ì—°ê²°ë˜ë©°, ì‹¤ì œ ë©´ì ‘ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.

## ğŸŒ Gradio ì¸í„°í˜ì´ìŠ¤ ì˜ˆì‹œ
```python
import gradio as gr

def interview_run(file, answer):
    state = preProcessing_Interview(file.name)
    state["current_answer"] = answer
    state = graph.invoke(state)
    return state["current_question"], state["evaluation"][0]["í‰ê°€ì— ëŒ€í•œ ì´ìœ "]

iface = gr.Interface(
    fn=interview_run,
    inputs=["file", "textbox"],
    outputs=["text", "text"],
    title="AI ë©´ì ‘ê´€ Agent v2.0"
)

iface.launch()
```
## ğŸ“Œ ê³ ë„í™” í•µì‹¬ ê¸°ëŠ¥
âœ… Resume ë¶„ì„ ê³ ë„í™” (ìš”ì•½ + í‚¤ì›Œë“œ + ì¤‘ìš”ë„ + íŠ¸ë¦¬ê±° íƒì§€)

âœ… ì§ˆë¬¸ ì „ëµ 3ë¶„ì•¼ ì„¤ì • + Vector DB ìœ ì‚¬ì§ˆë¬¸ ì°¸ì¡°

âœ… ë‹µë³€ í‰ê°€ í›„ reflectionìœ¼ë¡œ í‰ê°€ í’ˆì§ˆ ê²€í† 

âœ… ì „ëµ ìˆœí™˜ ë°©ì‹ ë©´ì ‘ íë¦„ + í‰ê°€ê¸°ë°˜ ì¢…ë£Œ/ì‹¬í™” ì „í™˜

âœ… ì¢…í•© í”¼ë“œë°± ìë™ ìƒì„±

## ğŸ“ì°¸ê³  ê¸°ìˆ 
OpenAI GPT-4o-mini

LangChain, LangGraph

Gradio (HuggingFace)

Chroma Vector DB

## ğŸ’¬ ì‹¤í–‰ ì˜ˆì‹œ
```plaintext
ë©´ì ‘ê´€: ì§€ì›ìë‹˜ì˜ ìê¸°ì†Œê°œë¥¼ ë“£ê³ , ê·¸ ì¤‘ì—ì„œ ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ê²½í—˜ì„ ìì„¸íˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?

ì§€ì›ì: ëŒ€í•™ ì‹œì ˆ ë™ì•„ë¦¬ í™œë™ì—ì„œ íŒ€ í”„ë¡œì íŠ¸ë¥¼ ì£¼ë„í•˜ë©°...

í”¼ë“œë°±: ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ì˜ ì„¤ëª…í•˜ì…¨ìŠµë‹ˆë‹¤. ê²°ê³¼ì— ëŒ€í•œ ìˆ˜ì¹˜ë‚˜ ì„±ê³¼ë¥¼ ë”í•˜ë©´ ë” ì¢‹ê² ìŠµë‹ˆë‹¤.

ì ìˆ˜: 85/100 - ê²½í—˜ì´ ì˜ ë“œëŸ¬ë‚¨, êµ¬ì²´ì„± ë³´ì™„ í•„ìš”
```
## ğŸ“Š ì‹¤ì œ ì„œë¹„ìŠ¤ í™”ë©´
![ì„œë¹„ìŠ¤ ì‹¤ì œ í™”ë©´](./ai_interview_ui.png)

